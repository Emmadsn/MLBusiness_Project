{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "NHOfkKPvMPpu",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NHOfkKPvMPpu",
        "outputId": "47f0fa5a-b15e-4dff-c02e-e02b323af4d9"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "eAD-4RsLAqfr",
      "metadata": {
        "id": "eAD-4RsLAqfr"
      },
      "source": [
        "## **Machine Learning for Business II - Project**\n",
        "## Business Use Case: Sentiment Analysis on a Disneyland dataset\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "96191029",
      "metadata": {},
      "source": [
        "# **BASELINE**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "877f18d8",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import necessary libraries and modules\n",
        "\n",
        "# Pandas is used for data manipulation and analysis. \n",
        "# Pandas provides data structures like DataFrame\n",
        "import pandas as pd\n",
        "\n",
        "# Import the train_test_split function from scikit-learn\n",
        "# train_test_split is used to split the dataset into training and testing sets.\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Import PyTorch for building and training the model\n",
        "# Torch is a deep learning library that provides tensor operations and support for building neural networks.\n",
        "import torch\n",
        "\n",
        "# Import DataLoader and Dataset from PyTorch\n",
        "# DataLoader is used to efficiently load the dataset in batches during training.\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "\n",
        "# Import BERT components from the Hugging Face transformers library\n",
        "# - BertTokenizer is used to convert raw text into tokens that BERT can understand.\n",
        "# - BertForSequenceClassification is a pre-trained BERT model specifically for text classification tasks.\n",
        "# - AdamW is an optimizer used to adjust the model's weights based on the gradients calculated during training.\n",
        "# - BertConfig is used to configure the BERT model, such as specifying the number of output labels.\n",
        "from transformers import BertTokenizer, BertForSequenceClassification, AdamW, BertConfig\n",
        "\n",
        "# Import the learning rate scheduler for training\n",
        "# get_linear_schedule_with_warmup is used to adjust the learning rate over time.\n",
        "# It starts with a warm-up phase, where the learning rate gradually increases, then decreases linearly.\n",
        "from transformers import get_linear_schedule_with_warmup\n",
        "\n",
        "# Import NumPy for numerical computations\n",
        "# NumPy is a library that provides support for arrays and numerical operations such as averaging losses.\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fbbf9564",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load the dataset from an Excel file\n",
        "# The dataset is stored in an Excel file and loaded into a Pandas DataFrame (df).\n",
        "df = pd.read_excel(\"Dataset ML project - Disney Reviews - Cleaned.xlsx\")\n",
        "\n",
        "# Select the relevant columns for training\n",
        "# - 'X' will store the text content of the reviews, which will be used as input features for the model.\n",
        "# - 'y' will store the sentiment labels (target variable) for each review, which the model will try to predict.\n",
        "X = df['Content']            # Column containing the review text\n",
        "y = df['feeling cont']        # Column containing the sentiment labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1ce69e0a",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Map the sentiment labels to integers\n",
        "# In order for the machine learning model to work with the labels, we need to convert the text labels into numbers.\n",
        "# This is called label encoding, where each unique sentiment label is assigned a numerical value.\n",
        "# The label mapping here is as follows:\n",
        "# - 'négatif' (negative) is mapped to 0\n",
        "# - 'neutre' (neutral) is mapped to 1\n",
        "# - 'positif' (positive) is mapped to 2\n",
        "# - 'mitigé' (mixed) is mapped to 3\n",
        "label_map = {'négatif': 0, 'neutre': 1, 'positif': 2, 'mitigé': 3}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e272ce47",
      "metadata": {},
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "# Load a pre-trained tokenizer for BERT\n",
        "# The tokenizer is responsible for converting raw text into token IDs that BERT can understand.\n",
        "# 'bert-base-uncased' refers to a pre-trained version of BERT that converts all text to lowercase (ignores case).\n",
        "# This pre-trained tokenizer has already learned how to split words into tokens based on large text datasets.\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "948d176b",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Split the dataset into training and test sets\n",
        "# 'train_test_split' is a function from scikit-learn that splits the data into two sets: one for training and one for testing.\n",
        "# - X: The features (in this case, the text data).\n",
        "# - y: The labels (in this case, the sentiment labels).\n",
        "# - test_size=0.2: 20% of the data will be used as the test set, while 80% will be used for training.\n",
        "# - random_state=42: This ensures that the split is reproducible (the same random split will occur every time).\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Print the shape of the training data (number of samples and features)\n",
        "# This will show the number of samples in the training set after the split.\n",
        "print(X_train.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "30251074",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load a pre-trained tokenizer for BERT\n",
        "# The tokenizer is responsible for converting text into the token format required by BERT.\n",
        "# 'bert-base-uncased' refers to a pre-trained version of BERT where all text is converted to lowercase (ignores case sensitivity).\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "\n",
        "# Tokenize the training texts\n",
        "# The tokenizer converts the input text into token IDs, attention masks, and other features that BERT needs for processing.\n",
        "train_encodings = tokenizer(\n",
        "    X_train.tolist(),             # Convert the training data (X_train) to a list of strings for tokenization\n",
        "    max_length=350,               # Maximum sequence length: sequences longer than 350 tokens will be truncated\n",
        "    padding='max_length',         # Add padding to sequences shorter than 350 tokens so all sequences have the same length\n",
        "    truncation=True,              # Truncate sequences that are longer than the maximum length (350 tokens)\n",
        "    return_tensors='pt'           # Return the output as PyTorch tensors for model training\n",
        ")\n",
        "\n",
        "# Tokenize the test texts\n",
        "# The same tokenization process is applied to the test data (X_test).\n",
        "test_encodings = tokenizer(\n",
        "    X_test.tolist(),              # Convert the test data (X_test) to a list of strings for tokenization\n",
        "    max_length=350,               # Maximum sequence length: same as for the training data\n",
        "    padding='max_length',         # Add padding to ensure consistent length\n",
        "    truncation=True,              # Truncate sequences that exceed the maximum length\n",
        "    return_tensors='pt'           # Return the output as PyTorch tensors for model evaluation\n",
        ")\n",
        "\n",
        "# Convert text labels into numerical labels\n",
        "# The sentiment labels (text) are converted into numerical format, as the model works with numbers.\n",
        "# 'label_map' is a dictionary that maps text labels (e.g., 'positive', 'negative') to numbers (e.g., 0, 1, 2, 3).\n",
        "test_labels = [label_map[label] for label in y_test]\n",
        "train_labels = [label_map[label] for label in y_train]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0a8765e3",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Convert the labels into PyTorch tensors\n",
        "# Tensors are a data structure used in PyTorch to store data in multi-dimensional arrays.\n",
        "# Here, we are converting the training and test labels into tensors that the model can use during training and evaluation.\n",
        "train_labels_tensor = torch.tensor(train_labels)\n",
        "test_labels_tensor = torch.tensor(test_labels)\n",
        "\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "import torch\n",
        "\n",
        "# Create TensorDatasets with input_ids, attention_mask, and labels\n",
        "# A TensorDataset is a PyTorch structure that groups together the input data (input_ids and attention_mask) with their corresponding labels.\n",
        "# This structure will be used to feed the model both the text inputs and the expected labels during training and testing.\n",
        "X_test_dataset = TensorDataset(test_encodings['input_ids'], test_encodings['attention_mask'], test_labels_tensor)\n",
        "X_train_dataset = TensorDataset(train_encodings['input_ids'], train_encodings['attention_mask'], train_labels_tensor)\n",
        "\n",
        "# Create DataLoaders for the test and training datasets\n",
        "# DataLoader is a PyTorch class that allows us to efficiently load the data in small batches during training and evaluation.\n",
        "# - batch_size=16: This means that 16 samples will be processed at a time.\n",
        "# - shuffle=True: This ensures the data is shuffled before each epoch, which helps improve model generalization.\n",
        "X_test_loader = DataLoader(X_test_dataset, batch_size=16, shuffle=True)\n",
        "X_train_loader = DataLoader(X_train_dataset, batch_size=16, shuffle=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "00e3a624",
      "metadata": {},
      "source": [
        "### Define hyperparameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "61a0fa61",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define hyperparameters\n",
        "# These are manually chosen values for the number of epochs, learning rate, and batch size.\n",
        "num_epochs = 1           # Number of epochs: how many times the model will see the entire dataset during training\n",
        "learning_rate = 1e-5     # Learning rate: controls how fast the model updates its weights during training\n",
        "batch_size = 15          # Batch size: how many samples are processed at one time during training\n",
        "\n",
        "# Load the pre-trained BERT model for sequence classification\n",
        "# This BERT model ('bert-base-uncased') has been pre-trained, but we are not yet fine-tuning it.\n",
        "# The 'num_labels=4' parameter indicates that we are classifying the input into 4 categories (e.g., positive, negative, neutral, mixed).\n",
        "model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=4)\n",
        "\n",
        "# Define the optimizer (AdamW) to adjust the model's weights based on the gradients\n",
        "# The optimizer is initialized with the model's parameters and the chosen learning rate\n",
        "optimizer = AdamW(model.parameters(), lr=learning_rate)\n",
        "\n",
        "# Check if a GPU (CUDA) is available; if not, use the CPU\n",
        "# Using a GPU can significantly speed up the training process\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Move the model to the appropriate device (GPU or CPU)\n",
        "model = model.to(device)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bf3e763e",
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.metrics import recall_score\n",
        "import torch\n",
        "\n",
        "# Check if a GPU (CUDA) is available, otherwise use the CPU\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "\n",
        "def train_test_model(model, train_loader, test_loader, optimizer, num_epochs):\n",
        "    avg_loss = []  # List to store the average loss for each epoch\n",
        "\n",
        "    # Train the model\n",
        "    for epoch in range(num_epochs):  # For example, over 2 epochs\n",
        "        model = model.to(device)  # Move the model to the appropriate device (GPU or CPU)\n",
        "        model.train()  # Put the model in training mode\n",
        "        total_loss = 0  # Initialize total loss for the epoch\n",
        "\n",
        "        # Loop over batches in the training data\n",
        "        for batch in train_loader:\n",
        "            optimizer.zero_grad()  # Reset gradients before each batch\n",
        "\n",
        "            # Unpack the data from the batch\n",
        "            input_ids, attention_mask, labels = batch\n",
        "\n",
        "            # Move data to the appropriate device\n",
        "            input_ids = input_ids.to(device)\n",
        "            attention_mask = attention_mask.to(device)\n",
        "            labels = labels.to(device)\n",
        "\n",
        "            # Forward pass through the model\n",
        "            outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
        "            loss = outputs.loss  # Calculate the loss\n",
        "\n",
        "            # Backpropagation to compute gradients\n",
        "            loss.backward()\n",
        "            optimizer.step()  # Update the model's parameters\n",
        "\n",
        "            # Add the loss for this batch to the total loss\n",
        "            total_loss += loss.item()\n",
        "\n",
        "        # Calculate and store the average loss for this epoch\n",
        "        avg_loss.append(total_loss / len(train_loader))\n",
        "\n",
        "    # Evaluate the model on the test data\n",
        "    model.eval()  # Put the model in evaluation mode (no training)\n",
        "    y_true, y_pred = [], []  # Lists to store true labels and predictions\n",
        "    total_loss = 0  # Initialize total loss for evaluation\n",
        "\n",
        "    # Disable gradient computation during evaluation\n",
        "    with torch.no_grad():\n",
        "        for batch in test_loader:\n",
        "            input_ids, attention_mask, labels = batch\n",
        "\n",
        "            # Forward pass through the model\n",
        "            outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
        "\n",
        "            # Get the loss\n",
        "            loss = outputs.loss\n",
        "            total_loss += loss.item()\n",
        "\n",
        "            # Get the model's predictions\n",
        "            logits = outputs.logits\n",
        "            predictions = torch.argmax(logits, dim=-1)  # Get the class with the highest score\n",
        "\n",
        "            # Append the true labels and predictions to the lists\n",
        "            y_true.extend(labels.cpu().tolist())\n",
        "            y_pred.extend(predictions.cpu().tolist())\n",
        "\n",
        "    # Calculate the average loss for the evaluation\n",
        "    avg_loss = total_loss / len(test_loader)\n",
        "\n",
        "    # Calculate the recall for each class\n",
        "    recall = recall_score(y_true, y_pred, average=None)\n",
        "\n",
        "    # Calculate the class-wise error rate (1 - recall)\n",
        "    class_error_rate = 1 - recall\n",
        "\n",
        "    # Return the average loss, recall, and class-wise error rate\n",
        "    return avg_loss, class_error_rate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2c610ec5",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test the pre-trained BERT model on the test set\n",
        "# The function 'train_test_model' trains the model on the training set and evaluates it on the test set.\n",
        "# It returns the average loss (avg_loss) and the class-wise error rate (class_error_rate).\n",
        "avg_loss, class_error_rate = train_test_model(model, X_train_loader, X_test_loader, optimizer, num_epochs)\n",
        "\n",
        "# Display the results of the model's performance\n",
        "print(f\"Average Loss: {avg_loss}\")\n",
        "print(f\"Class-wise Error Rate: {class_error_rate}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8b75ff00",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Save the baseline model results in a text file\n",
        "# This file will keep a record of the performance of the baseline model\n",
        "# The results include the final average loss (avg_loss) and the class-wise error rate (class_error_rate)\n",
        "\n",
        "with open(\"Baseline score.txt\", \"w\") as f:\n",
        "    # Write the final loss to the file\n",
        "    f.write(f\"Final Loss: {avg_loss}\\n\")\n",
        "\n",
        "    # Write the class-wise error rate (as a list) to the file\n",
        "    # The class-wise error rate is important for evaluating the model's performance on each sentiment category\n",
        "    f.write(f\"Class-wise error rate: {class_error_rate.tolist()}\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "45291498",
      "metadata": {},
      "source": [
        "# **PIPELINE**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "panl8ybWsX5S",
      "metadata": {
        "id": "panl8ybWsX5S"
      },
      "source": [
        "## Step 1 - Loading of the dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f12d0808",
      "metadata": {},
      "outputs": [],
      "source": [
        "#%pip install optuna"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "f7bdd998-d099-4dc9-b795-cb176d9db4a5",
      "metadata": {
        "id": "f7bdd998-d099-4dc9-b795-cb176d9db4a5"
      },
      "outputs": [],
      "source": [
        "# Import necessary libraries and modules\n",
        "\n",
        "# Pandas is used for data manipulation and analysis. \n",
        "# Pandas provides data structures like DataFrame\n",
        "import pandas as pd\n",
        "\n",
        "# Importing train_test_split from scikit-learn to split the dataset into training and testing sets\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import recall_score\n",
        "\n",
        "# Importing PyTorch for tensor operations (multidimensional arrays which are the core data structures for neural networks) and providing support for building and training deep learning models\n",
        "import torch\n",
        "\n",
        "# Importing DataLoader and Dataset from PyTorch for creating data pipelines and managing datasets efficiently\n",
        "from torch.utils.data import DataLoader, Dataset, Subset\n",
        "\n",
        "# Importing components from Hugging Face Transformers library for BERT tokenization and sequence classification\n",
        "from transformers import BertTokenizer, BertForSequenceClassification, AdamW, BertConfig\n",
        "\n",
        "# Importing scheduler to manage the learning rate schedule during training\n",
        "from transformers import get_linear_schedule_with_warmup\n",
        "\n",
        "# Importing numpy for efficient numerical operations, especially for working with arrays\n",
        "import numpy as np\n",
        "\n",
        "import optuna"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "c3d7f39b-7ccf-457e-b1a2-709dda15c388",
      "metadata": {
        "id": "c3d7f39b-7ccf-457e-b1a2-709dda15c388"
      },
      "outputs": [],
      "source": [
        "# Load the dataset from an Excel file\n",
        "df = pd.read_excel(\"/content/drive/MyDrive/ML business - project/Dataset ML project - Disney Reviews - Cleaned.xlsx\")\n",
        "\n",
        "# Select the relevant columns for training the model\n",
        "# 'X' will store the content of the reviews (features)\n",
        "# 'y' will store the sentiment labels (target variable)\n",
        "X = df['Content']\n",
        "y = df['feeling cont']\n",
        "\n",
        "# Split the dataset into training and test sets\n",
        "# X_train, X_test: The training and test data (features)\n",
        "# y_train, y_test: The corresponding training and test labels (sentiments)\n",
        "# test_size=0.2: This means 20% of the data will be used for the test set, and 80% for the training set.\n",
        "# random_state=42: This is used to ensure that the split is the same every time the code is run (reproducibility)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Create a mapping of sentiment labels to integers\n",
        "# We convert the text labels (e.g., 'négatif') into numerical values so the model can process them.\n",
        "# 'négatif' -> 0, 'neutre' -> 1, 'positif' -> 2, 'mitigé' -> 3\n",
        "label_map = {'négatif': 0, 'neutre': 1, 'positif': 2, 'mitigé': 3}\n",
        "\n",
        "# Convert the text labels in 'y_train' into their corresponding numerical values\n",
        "# For example, if y_train contains 'négatif', it will be converted to 0, and so on.\n",
        "# This allows the model to use the labels as numbers for training.\n",
        "labels = [label_map[label] for label in y_train]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "xW9WLs8-KFtc",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xW9WLs8-KFtc",
        "outputId": "b9723784-3f7a-4e1b-9b31-94f1d72669ee"
      },
      "outputs": [],
      "source": [
        "# Load the pre-trained BERT tokenizer\n",
        "# This tokenizer will convert the text data into a format that BERT can understand\n",
        "# 'bert-base-uncased' means we are using the BERT model that ignores case sensitivity (lowercasing all text)\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "\n",
        "# Tokenize the text data in 'X_train'\n",
        "# The tokenizer converts each text review into a sequence of numbers (called tokens) that represent the words in BERT’s vocabulary.\n",
        "encodings = tokenizer(\n",
        "    X_train.tolist(),             # Convert the training data to a list format for tokenization\n",
        "    max_length=350,               # Maximum sequence length for each review (350 tokens max)\n",
        "    padding='max_length',         # Pad shorter sequences with zeros to make all sequences the same length\n",
        "    truncation=True,              # Cut off sequences that are longer than 350 tokens\n",
        "    return_tensors='pt'           # Return the output as PyTorch tensors, which are used in model training\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "4YJOgj85Lx1y",
      "metadata": {
        "id": "4YJOgj85Lx1y"
      },
      "outputs": [],
      "source": [
        "# Import necessary libraries\n",
        "import torch\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "\n",
        "# Transform the labels into PyTorch tensors\n",
        "# Tensors are multi-dimensional arrays that PyTorch uses to store data.\n",
        "# In this case, we are converting our sentiment labels (numbers) into a tensor format that the model can work with.\n",
        "labels_tensor = torch.tensor(labels)\n",
        "\n",
        "# Create a TensorDataset using the tokenized inputs and labels\n",
        "# A TensorDataset is a PyTorch structure that allows us to group input data (input_ids, attention_mask) with their corresponding labels.\n",
        "# - 'input_ids': The tokenized form of the text (converted words into numbers).\n",
        "# - 'attention_mask': Indicates which tokens should be attended to (helps the model ignore padding tokens).\n",
        "# - 'labels_tensor': The labels (e.g., positive, negative) converted into tensor format.\n",
        "train_dataset = TensorDataset(encodings['input_ids'], encodings['attention_mask'], labels_tensor)\n",
        "\n",
        "# Create a DataLoader for the training dataset\n",
        "# The DataLoader splits the dataset into small batches (mini-batches) of 16 samples each.\n",
        "# It loads the data in batches to efficiently train the model, and we can also shuffle the data during training if needed.\n",
        "# - batch_size=16: This means that during each iteration of training, 16 samples will be fed to the model at a time.\n",
        "# - shuffle=False: Data will not be shuffled in this case, but you can set this to True to shuffle the data before each epoch.\n",
        "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=False)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6IE6a5zgJVCV",
      "metadata": {
        "id": "6IE6a5zgJVCV"
      },
      "source": [
        "## Step 2 - Training\n",
        "Training of a whole comment analysis model on a manually generated training dataset\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "u15uuarz8MzE",
      "metadata": {
        "id": "u15uuarz8MzE"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import KFold\n",
        "\n",
        "# Define the number of splits for cross-validation\n",
        "# K-Fold Cross-Validation is a technique where the dataset is split into 'k' equal parts (or folds).\n",
        "# The model is trained 'k' times, each time using a different fold as the test set and the remaining folds as the training set.\n",
        "# This helps to evaluate the model's performance more reliably by using different splits of the data.\n",
        "# In this case, we are using 5 splits, meaning the dataset will be divided into 5 parts, and the model will be trained and evaluated 5 times.\n",
        "kf = KFold(n_splits=5)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ba543f3c-d7df-471c-a946-90b86006772e",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ba543f3c-d7df-471c-a946-90b86006772e",
        "outputId": "5f1b3304-9966-4ea1-fcd7-14dadec92571"
      },
      "outputs": [],
      "source": [
        "# Initialize the BERT model for sequence classification\n",
        "# We are using the pre-trained 'bert-base-uncased' model, which is designed to ignore case (lowercase everything).\n",
        "# The configuration specifies that the model will classify text into 4 sentiment categories (e.g., positive, negative, neutral, mixed).\n",
        "config = BertConfig.from_pretrained('bert-base-uncased', num_labels=4)\n",
        "\n",
        "# Load the pre-trained BERT model with the specified configuration\n",
        "# 'BertForSequenceClassification' is a version of BERT designed for classification tasks.\n",
        "# This loads the model with the configuration we just set (which includes 4 possible output labels).\n",
        "model = BertForSequenceClassification.from_pretrained('bert-base-uncased', config=config)\n",
        "\n",
        "# Initialize the optimizer for model training\n",
        "# 'AdamW' is a variant of the Adam optimizer commonly used for training transformers like BERT.\n",
        "# We specify the model's parameters and set the learning rate to 5e-5, which controls how much the model's weights are adjusted during training.\n",
        "# A small learning rate like 5e-5 ensures that the model learns gradually to avoid large, unstable updates.\n",
        "optimizer = AdamW(model.parameters(), lr=5e-5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "id": "k-VaXXFU1zIx",
      "metadata": {
        "id": "k-VaXXFU1zIx"
      },
      "outputs": [],
      "source": [
        "# Define the objective function to be optimized by Optuna\n",
        "def objective(trial):\n",
        "    # Define the hyperparameter search space\n",
        "\n",
        "    # Suggest possible batch sizes from a list of values\n",
        "    batch_size = trial.suggest_categorical(\"batch_size\", [10, 20, 30, 50])\n",
        "\n",
        "    # Suggest learning rate on a log scale between 1e-5 and 5e-5\n",
        "    learning_rate = trial.suggest_loguniform(\"learning_rate\", 1e-5, 5e-5)\n",
        "\n",
        "    # Suggest the number of epochs from a set of values (2, 3, or 5)\n",
        "    num_epochs = trial.suggest_int(\"num_epochs\", 2, 3, 5)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "id": "JDIhtN6N5S-Q",
      "metadata": {
        "id": "JDIhtN6N5S-Q"
      },
      "outputs": [],
      "source": [
        "# Define the objective function for Optuna to optimize\n",
        "# Optuna will adjust hyperparameters to minimize the error rate across multiple trials.\n",
        "def objective(trial):\n",
        "    # Define the hyperparameter search space:\n",
        "\n",
        "    # Suggest possible batch sizes (how many samples are processed at once) from a list of values\n",
        "    batch_size = trial.suggest_categorical(\"batch_size\", [10, 20, 30, 50])\n",
        "\n",
        "    # Suggest a learning rate on a logarithmic scale between 1e-5 and 5e-5\n",
        "    # A learning rate controls how fast the model updates its weights\n",
        "    learning_rate = trial.suggest_float(\"learning_rate\", 1e-5, 5e-5, log=True)\n",
        "\n",
        "    # Suggest the number of epochs (how many times the model will see the entire dataset) from a range\n",
        "    # The number of epochs is selected from 1 to 5 in steps of 2 (so possible values are 1, 3, 5)\n",
        "    num_epochs = trial.suggest_int(\"num_epochs\", 1, 5, step=2)\n",
        "\n",
        "    avg_loss = []  # To store the average loss for each fold during cross-validation\n",
        "    avg_error_rate = []  # To store the average error rate for each fold\n",
        "\n",
        "    # Perform cross-validation: k-folds split the dataset into training and test subsets multiple times\n",
        "    for fold, (train_idx, test_idx) in enumerate(kf.split(train_dataset)):\n",
        "        print(f'Fold {fold + 1}')  # Show the current fold number\n",
        "\n",
        "        # Initialize a new BERT model for sequence classification for each fold\n",
        "        model = BertForSequenceClassification.from_pretrained('bert-base-uncased', config=config)\n",
        "\n",
        "        # Define the optimizer with the suggested learning rate\n",
        "        optimizer = AdamW(model.parameters(), lr=learning_rate)\n",
        "\n",
        "        # Create subsets for training and testing based on the fold's indices\n",
        "        train_subset = Subset(train_dataset, train_idx)\n",
        "        test_subset = Subset(train_dataset, test_idx)\n",
        "\n",
        "        # Create DataLoaders to load the training and testing data in batches\n",
        "        # Batch size determines how many samples are processed at once during training\n",
        "        train_dataloader = DataLoader(train_subset, batch_size=batch_size, shuffle=True)\n",
        "        test_dataloader = DataLoader(test_subset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "        # Train the model on the current fold\n",
        "        model.train()  # Switch the model to training mode\n",
        "        total_loss = 0  # To accumulate the total loss over all batches\n",
        "        for epoch in range(num_epochs):  # Loop over the number of epochs\n",
        "            for batch in train_dataloader:\n",
        "                optimizer.zero_grad()  # Reset gradients before processing each batch\n",
        "\n",
        "                # Extract input data from the batch\n",
        "                input_ids, attention_mask, labels = batch\n",
        "\n",
        "                # Forward pass: compute model predictions and the loss\n",
        "                outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
        "                loss = outputs.loss  # The loss measures how far off the predictions are\n",
        "\n",
        "                # Backpropagation: compute gradients and update model weights\n",
        "                loss.backward()\n",
        "                optimizer.step()  # Apply the optimizer to update model parameters\n",
        "\n",
        "                # Add the loss from this batch to the total loss\n",
        "                total_loss += loss.item()\n",
        "\n",
        "        # Calculate the average loss for this fold and add it to the list\n",
        "        avg_loss.append(total_loss / len(train_dataloader))\n",
        "\n",
        "        # Evaluate the model on the test set (current fold)\n",
        "        model.eval()  # Switch the model to evaluation mode (no training)\n",
        "        y_true = []  # True labels\n",
        "        y_pred = []  # Model predictions\n",
        "\n",
        "        # Turn off gradient calculations during evaluation to save memory and computation\n",
        "        with torch.no_grad():\n",
        "            for batch in test_dataloader:\n",
        "                input_ids, attention_mask, labels = batch\n",
        "\n",
        "                # Forward pass through the model to make predictions\n",
        "                outputs = model(input_ids, attention_mask=attention_mask)\n",
        "                logits = outputs.logits  # Logits are raw model outputs before being turned into probabilities\n",
        "                predictions = torch.argmax(logits, dim=-1)  # Get the predicted class (highest score)\n",
        "\n",
        "                # Collect the true labels and predicted labels\n",
        "                y_true.extend(labels.cpu().tolist())\n",
        "                y_pred.extend(predictions.cpu().tolist())\n",
        "\n",
        "                # Print a sample of true and predicted labels to monitor performance\n",
        "                for true_label, pred_label in zip(y_true[:10], y_pred[:10]):\n",
        "                    print(f\"True label: {true_label}, Prediction: {pred_label}\")\n",
        "\n",
        "        # Calculate recall for each class (how well the model identifies true positives)\n",
        "        recall = recall_score(y_true, y_pred, average=None)\n",
        "\n",
        "        # Calculate the error rate for each class (1 - recall)\n",
        "        error_rate = 1 - recall\n",
        "        avg_error_rate.append(np.mean(error_rate))  # Store the average error rate across classes\n",
        "        print(f\"Trial {trial.number} - Class-wise error rate:\", error_rate)\n",
        "\n",
        "        # Optionally, save the metrics for each trial in a file for further analysis\n",
        "        with open(\"score.txt\", \"a\") as f:\n",
        "            f.write(f\"Trial {trial.number + 1} - Avg Loss: {np.mean(avg_loss):.4f}, Avg Class-wise error rate: {np.mean(avg_error_rate):.4f}\\n\")\n",
        "\n",
        "    # Return the average error rate across all folds to Optuna for optimization\n",
        "    return np.mean(avg_error_rate)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Pkz1XkLV2Hbg",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pkz1XkLV2Hbg",
        "outputId": "072606b6-2eb8-48ea-f020-a8a16d20d0dc"
      },
      "outputs": [],
      "source": [
        "# Set up the Optuna study for hyperparameter optimization\n",
        "# \"minimize\" indicates that the goal is to minimize the error rate (since lower error means better performance)\n",
        "# This means Optuna will try to find the set of hyperparameters that results in the lowest error rate.\n",
        "study = optuna.create_study(direction=\"minimize\")\n",
        "\n",
        "# Run the optimization process for a defined number of trials (in this case, 1 trial for testing purposes)\n",
        "# The optimization will explore different combinations of hyperparameters by calling the 'objective' function.\n",
        "# Typically, a larger number of trials (e.g., 20 or 50) is used to find the best results.\n",
        "study.optimize(objective, n_trials=1)\n",
        "\n",
        "# Display the best hyperparameters found by Optuna\n",
        "print(\"Best trial:\")  # Print the results of the best trial\n",
        "trial = study.best_trial  # Get the best trial (the one with the lowest error rate)\n",
        "print(f\"  Best Error Rate: {trial.value}\")  # Print the lowest error rate found during optimization\n",
        "print(f\"  Best Hyperparameters: {trial.params}\")  # Print the best hyperparameter combination\n",
        "\n",
        "# Train the final model using the best hyperparameters found by Optuna\n",
        "# Extract the best hyperparameters (batch size, learning rate, and number of epochs) from the best trial\n",
        "best_batch_size = trial.params[\"batch_size\"]\n",
        "best_learning_rate = trial.params[\"learning_rate\"]\n",
        "best_num_epochs = trial.params[\"num_epochs\"]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "kXwZORa96Esa",
      "metadata": {
        "id": "kXwZORa96Esa"
      },
      "outputs": [],
      "source": [
        "import mlflow\n",
        "from mlflow.models import infer_signature\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "# Initialize a Logistic Regression model from scikit-learn\n",
        "# Logistic regression is a simple classification model used for binary or multiclass classification tasks.\n",
        "logistic_regressor = LogisticRegression()\n",
        "\n",
        "# It's a good practice to infer the model's signature.\n",
        "# The \"signature\" captures the input and output structure of the model, including data types and shapes.\n",
        "# This is useful for tracking and documenting the model in MLflow for later use (especially when deploying it).\n",
        "signature = infer_signature(X, logistic_regressor.predict(X))\n",
        "\n",
        "# Log the model into MLflow\n",
        "# MLflow provides a platform to manage and track machine learning models. When logging a model, it saves:\n",
        "# - The model itself (in this case, the Logistic Regression model).\n",
        "# - The inferred signature (which shows the input/output structure).\n",
        "# - Additional metadata about the model.\n",
        "# This allows the model to be easily tracked, versioned, and potentially deployed later.\n",
        "mlflow.sklearn.log_model(logistic_regressor, signature=signature)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "JxJ5K_NaJllq",
      "metadata": {
        "id": "JxJ5K_NaJllq"
      },
      "source": [
        "## Step 3 - Testing"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "syS9V628r9gi",
      "metadata": {
        "id": "syS9V628r9gi"
      },
      "source": [
        "## Step 4 - Analysis of Metrics"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fKZKnys6kQSH",
      "metadata": {
        "id": "fKZKnys6kQSH"
      },
      "source": [
        "**BONUS**\n",
        "1) If there is proper Experimentation Tracking.\n",
        "2) If there is a documented HTTP API that serves the trained model."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "optimization",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
